# Logstash config for reading a Toil LSF log file from the workdir

input {

  file {
    type => "meta"
    path => ["${TOIL_WORK_PATH}/*/.run.info"]
    start_position => "beginning"
    sincedb_path => "sincedb"
    codec => json
  }
}

filter {
  if [type] == "meta" {
    if ![run_info] {
      mutate {
        add_field => {
          "run_info" => {
          }
        }
      }
    }
    mutate {
      copy => {
        "[pipeline_id]" => "[run_info][pipeline_id]"
        "[external_id]" => "[run_info][external_id]"
        "[run_id]" => "[run_info][run_id]"
        "[working_dir]" => "[run_info][working_dir]"
        "[pipeline_link]" => "[run_info][pipeline_link]"
        "[output_directory]" => "[run_info][output_directory]"
        "[job_store_location]" => "[run_info][job_store_location]"
      }
    }
    memcached {
      hosts => ["${MEMCACHED}"]
      set => {
        "[run_info]" => "%{run_id}_run_info"
      }
      ttl => 7890000
    }
    drop {
    }
  }
}

input {
  file {
    type => "log"
    path => ["${TOIL_WORK_PATH}/*/lsf.log"]
    sincedb_path => "${SINCEDB_PATH}"
    start_position => "beginning"
    stat_interval => "2 m"
    codec => multiline {
      # lines starting with whitespace get appened to previous entry
      patterns_dir => ["${PATTERNS_PATH}"]
      pattern => "(^%{TOIL_DATE_TIME}(?!.*Exiting the worker because of a failed job on host.*)|^Sender: LSF System|^Log from job|^\s*$)"
      negate => true
      what => "previous"
    }
  }
}
filter {
  if [type] == "log" {
    if [path] {
      grok {
        match => {
          "path" => ".*work/%{DATA:RUN_ID}/.*log"
        }
      }
    }
    if [RUN_ID] {
      memcached {
        hosts => ["${MEMCACHED}"]
        get => {
          "%{RUN_ID}_run_info" => "[run_info]"
        }

      }
      mutate {
        remove_field => ["RUN_ID"]
      }
    }



    if [message] =~ "Toil version 3" {
      drop {
      }
    }
    if [message] !~ "(.*<=========|.*Traceback.*)" {
      mutate {
        gsub => ["message", "(\n|\t)", ""]
      }
    }
    grok {
      patterns_dir => ["${PATTERNS_PATH}"]
      keep_empty_captures => true
      match => {
        "message" => [
        "%{TOIL_JOB_FAILED}",
        "%{TOIL_JOB_FAILED_2}",
        "%{TOIL_JOB_FAILED_3}",
        "%{TOIL_JOB_FAILED_4}",
        "%{TOIL_JOB_FAILED_5}",
        "%{TOIL_OPERATION_FAILED}",
        "%{TOIL_JAVASCRIPT_FAILED}",
        "%{TOIL_JOB_FAILED_LIST}",
        "%{TOIL_JOINING_LOG}",
        "%{TOIL_WORKER_NON_JAVASCRIPT_LOG}",
        "%{TOIL_WORKER_JAVASCRIPT_FAILED}",
        "%{TOIL_CHAIN_JOBS}",
        "%{TOIL_PROCESSING_JOB}",
        "%{TOIL_WORKING_JOB}",
        "%{TOIL_VERSION_AND_HOST}",
        "%{TOIL_JOB_COMPLETED}",
        "%{TOIL_ISSUED_JOB}",
        "%{TOIL_SAVING_GRAPH}",
        "%{TOIL_SINGLE_MACHINE_CORES}",
        "%{TOIL_NOT_CHAINING}",
        "%{TOIL_STATS_MESSAGE}",
        "%{TOIL_RESOLVED_MESSAGE}",
        "%{TOIL_LOADED_BODY}",
        "%{TOIL_REAL_TIME_SERVER}",
        "%{TOIL_RESULT_PROCESSED}",
        "%{TOIL_RUNNING_AND_PENDING}",
        "%{TOIL_FINISHED}",
        "%{TOIL_TRACEBACK}",
        "%{TOIL_CWL_WARNING}",
        "%{TOIL_CWL_WARNING_2}",
        "%{TOIL_EXTRA_COMMENT}",
        "%{TOIL_EXTRA_COMMENT_2}",
        "%{EMPTY_SPACE}",
        "%{TOIL_WORKER_LOG_HEADER}",
        "%{TOIL_SUCCESSFULLY_COMPLETED}",
        "%{TOIL_OUTPUT_FILES}",
        "%{TOIL_NO_LOG_FAILURE}",
        "%{TOIL_BATCH_SYSTEM_FAILURE}",
        "%{TOIL_BATCH_DESPITE_FAILURE}",
        "%{TOIL_CHAINING_FROM_JOB}",
        "%{TOIL_DOUBLE_MEMORY}",
        "%{TOIL_JOINING_REAL_TIME_LOGGER}",
        "%{LSF_JOB_COMMENT}",
        "%{LSF_JOB_COMMENT_2}",
        "%{LSF_JOB_COMMENT_3}",
        "%{LSF_JOB_SUMMARY}",
        "%{LSF_JOB_SUMMARY_2}",
        "%{LSF_JOB_SUMMARY_3}",
        "%{LSF_USAGE}",
        "%{LSF_EXIT}"
        ]
      }
    }

    if [host] {
      mutate {
        remove_field => ["host"]
      }
    }
    if [STATUS] == "failed" {
      mutate {
        add_field => {
          "LEVEL" => "ERROR"
        }
      }
    }
    if [STATUS] == "failure" {
      mutate {
        add_field => {
          "LEVEL" => "ERROR"
        }
      }
    }
    if [STATUS] == "errored" {
      mutate {
        add_field => {
          "LEVEL" => "ERROR"
        }
      }
    }
    if [ERROR_INFO] or [ERROR_TYPE] {
      mutate {
        add_field => {
          "LEVEL" => "ERROR"
        }
      }
    }

    if [NON_JAVASCRIPT_WORKER_LOG] {
      grok {
        patterns_dir => ["${PATTERNS_PATH}"]
        match => {
          "NON_JAVASCRIPT_WORKER_LOG" => [
          "%{TOIL_WORKER_FAILED}"
          ]
        }
      }
      mutate {
        remove_field => ["NON_JAVASCRIPT_WORKER_LOG"]
      }
    }

    if [path] {
      grok {
        match => {
          "path" => ".*work/%{DATA:RUN_ID}/.*log"
        }
      }
    }

    if [FAILED_RUN_LIST] {
      mutate {
        split => {
          "FAILED_RUN_LIST" => " '"
        }
      }
      grok {
        patterns_dir => ["${PATTERNS_PATH}"]
        match => {
          "FAILED_RUN_LIST" => [
          "%{TOIL_SINGLE_FAILED_JOB_META}"
          ]
        }
      }
    }
    if [SCRIPT_DATA] {
      mutate {
        gsub => ["SCRIPT_DATA", "\n\t\d+\s+", ""]
      }
      mutate {
        gsub => ["SCRIPT_DATA", "\"", "'"]
      }

    }
    if [COMMAND] {
      mutate {
        gsub => ["COMMAND", "(^\s+|\\\n\t\s+)", ""]
      }
    }
    if [JOBFILES] {
      mutate {
        split => {
          "JOBFILES" => "Downloaded file "
        }
      }
      grok {
        patterns_dir => ["${PATTERNS_PATH}"]
        match => {
          "JOBFILES" => [
          "%{TOIL_WORKER_FILE_NAME}"
          ]
        }
      }
    }
    if [HOUR] {
      mutate {
        add_field => {
          "timestamp" => "%{MONTH}.%{DAY}.%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}"
        }

      }
      date {
        match => [
        "timestamp",
        "MM.dd.YYYY HH:mm:ss"
        ]
        target => "date_time"
        remove_field => [
        "DAY",
        "MONTH",
        "YEAR",
        "HOUR",
        "MINUTE",
        "SECOND"
        ]
      }
    }

    if [ERROR] {
      mutate {
        remove_tag => ["_grokparsefailure"]
      }
    }
    if [STATUS] == "" {
      drop {
      }
    }
    if "_grokparsefailure" not in [tags] {
      mutate {
        remove_field => ["message"]
      }
    }

    if [NEW_MEMORY] {
      ruby {
        code => "event.set('MEMORY', (event.get('NEW_MEMORY').to_f / 1000000000).round(1).to_s)"
      }
      mutate {
        remove_field => ["NEW_MEMORY"]
      }
    }

    mutate {
      add_field => {
        "ddtags" => "${DD_TAGS}"
        "ddsource" => "logstash"
      }
    }

    ### Filter out log events ###
    if [CHAIN_DURATION] or [PERCENT_DISK] or [CLASS] == "toil.statsAndLogging" or [JOB] in ["CWLGather","CWLScatter","ResolveIndirect","CWLWorkflow"] or [STATUS] in ["Saving graph","Not chaining"] {
      drop {
      }
    }

    ### Add job info cache ###
    if [JOB_ID] {
      memcached {
        hosts => ["${MEMCACHED}"]
        get => {
          "%{[run_info][run_id]}_%{JOB_ID}_info" => "[job_info]"
        }
      }
      if ![job_info] {
        mutate {
          add_field => {
            "[job_info][history]" => {
            }
            "[job_info][info]" => {

            }

          }
        }
      }
    }

    ### Set time fields ###

    if [JOB_ID] and [CLASS] == "toil.job" and [SYMBOL] == "I" {
      mutate {
        add_field => {
          "STATUS" => "Processing"
        }

      }
    }

    if [JOB_ID] and [MEMORY] {
      mutate {
        add_field => {
          "STATUS" => "Submitted"
        }
      }
    }

    ### Unify error status

    if [STATUS] == "failure" or [STATUS] == "permanentFail" or [STATUS] == "errored" or [STATUS] == "failed" {
      mutate {
        update => {
          "STATUS" => "Failed"
        }
      }

    }

    ### Add info fields ###

    if [JOB_ID] {
      ruby {
        code => '
          info_keys = ["MEMORY","CORES","JOB","JOBNAME","COMMAND_NAME","CONTAINER","LSF_HOST","CWL_ERROR","EXTRA_COMMAND_INFO","ERROR","EXITCODE","EXITREASON"]
          info_keys.each do |key|
            event_value = event.get("[#{key}]")
            current_info_value = event.get("[job_info][info][#{key}]")
            if !event_value.nil? && current_info_value.nil?
              event.set("[job_info][info][#{key}]",event_value)
            end
          end
        '
      }


      ### Add history info ###
      if [STATUS] == "Submitted" or [STATUS] == "Failed" {
        ruby {
          code => '
            status_value = event.get("[STATUS]")
            timestamp = event.get("[timestamp]")
            current_history_key = "[job_info][history][#{status_value}" + "_str]"
            current_time_value = event.get(current_history_key)
            if current_time_value.nil?
              event.set(current_history_key, timestamp)
            else
              current_time_datetime = DateTime.strptime(current_time_value, "%m.%d.%Y %H:%M:%S")
              timestamp_datetime = DateTime.strptime(timestamp, "%m.%d.%Y %H:%M:%S")
              if timestamp_datetime > current_time_datetime
                event.set(current_history_key, timestamp)
              end
            end
            prev_time = nil
            key = nil
            if status_value == "Submitted"
              prev_time = "Processing"
              key = "processing_time"
            elsif status_value == "Failed"
              prev_time = "Submitted"
              key = "fail_time"
            end
            if !key.nil?
              prev_time_key = "[job_info][history][#{prev_time}" + "_str]"
              prev_time_value = event.get(prev_time_key)
              current_time_value = event.get(current_history_key)
              if !prev_time_value.nil? && !current_time_value.nil?
                prev_time_datetime = DateTime.strptime(prev_time_value, "%m.%d.%Y %H:%M:%S")
                current_time_datetime = DateTime.strptime(current_time_value, "%m.%d.%Y %H:%M:%S")
                delta = ((current_time_datetime.to_time.to_i  - prev_time_datetime.to_time.to_i  ) /60 ).to_i
                event.set("[job_info][history][#{key}]", delta)
              end
            end
          '
        }

      }
    }


    ### Set the job cache


    if [job_info] {
      memcached {
        hosts => ["${MEMCACHED}"]
        set => {
          "[job_info]" => "%{[run_info][run_id]}_%{JOB_ID}_info"
        }
        ttl => 7890000
      }
    }

    ## Set the dates in job_info into proper date objects
    if [job_info][history][Processing_str] {
      date {
        match => [
        "[job_info][history][Processing_str]",
        "MM.dd.YYYY HH:mm:ss"
        ]
        target => "[job_info][history][Processing]"
        remove_field => "[job_info][history][Processing_str]"
      }

    }
    if [job_info][history][Submitted_str] {
      date {
        match => [
        "[job_info][history][Submitted_str]",
        "MM.dd.YYYY HH:mm:ss"
        ]
        target => "[job_info][history][Submitted]"
        remove_field => "[job_info][history][Submitted_str]"
      }

    }
    if [job_info][history][Failed_str] {
      date {
        match => [
        "[job_info][history][Failed_str]",
        "MM.dd.YYYY HH:mm:ss"
        ]
        target => "[job_info][history][Failed]"
        remove_field => "[job_info][history][Failed_str]"
      }

    }

    #### SECURIRTY ####

    clone {
      clones => ["dog_log"]
    }


    if [type] == "dog_log" {

      if [COMMAND] {
        mutate {
          replace => {
            "COMMAND" => "Redacted for security reasons, please see %{path} for more info"
          }
        }
      }
      if [JOBFILES] {
        mutate {
          replace => {
            "JOBFILES" => "Redacted for security reasons, please see %{path} for more info"
          }
        }
      }
      if [FILES] {
        mutate {
          replace => {
            "FILES" => "Redacted for security reasons, please see %{path} for more info"
          }
        }
      }
    }
    mutate {
      remove_field => ["PATH"]
    }


    ruby {
      code => '
        event.to_hash.each { |k, v|
            if v.kind_of? String
                if v == ""
                    event.remove(k)
                end
            else
                if v == nil
                    event.remove(k)
                end
            end
        }
      '
    }
  }
}
output {
  if [type] == "dog_log" {
    datadog_logs {
      api_key => "${DATADOG_API_KEY}"
    }

  }

  if [LEVEL] == "ERROR" and [type] == "log" {
    file {
      path => "${LOGSTASH_OUTPUT_PATH}/logstash_output.json"
    }
  }

  if "_grokparsefailure" in [tags] and [type] == "log" {
    file {
      path => "${LOGSTASH_OUTPUT_PATH}/grok_parse_error.json"
    }
  }

  # stdout {
  #   codec => rubydebug
  # }

}
