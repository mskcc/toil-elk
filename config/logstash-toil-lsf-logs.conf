# Logstash config for reading a Toil LSF log file from the workdir

input {
  file {
    path => ["${TOIL_WORK_PATH}/*/lsf.log"]
    sincedb_path => "${SINCEDB_PATH}"
    start_position => "beginning"
    stat_interval => "2 m"
    codec => multiline {
        # lines starting with whitespace get appened to previous entry
      patterns_dir => ["${PATTERNS_PATH}"]
      pattern => "(^%{TOIL_DATE_TIME}(?!.*Exiting the worker because of a failed job on host.*)|^Sender: LSF System|^Log from job|^\s*$)"
      negate => true
      what => "previous"
    }
  }
}

filter{
  if [message] =~ "Toil version 3"{
    drop {}
  }
  if [message] !~ "(.*<=========|.*Traceback.*)" {
    mutate {
      gsub => ["message", "(\n|\t)", ""]
    }
  }
  grok {
    patterns_dir => ["${PATTERNS_PATH}"]
    keep_empty_captures => true
    match => {
    "message" => [
      "%{TOIL_JOB_FAILED}",
      "%{TOIL_JOB_FAILED_2}",
      "%{TOIL_JOB_FAILED_3}",
      "%{TOIL_JOB_FAILED_4}",
      "%{TOIL_JOB_FAILED_5}",
      "%{TOIL_OPERATION_FAILED}",
      "%{TOIL_JAVASCRIPT_FAILED}",
      "%{TOIL_JOB_FAILED_LIST}",
      "%{TOIL_JOINING_LOG}",   
      "%{TOIL_WORKER_NON_JAVASCRIPT_LOG}",
      "%{TOIL_WORKER_JAVASCRIPT_FAILED}",
      "%{TOIL_CHAIN_JOBS}",
      "%{TOIL_PROCESSING_JOB}",
      "%{TOIL_WORKING_JOB}",
      "%{TOIL_VERSION_AND_HOST}",
      "%{TOIL_JOB_COMPLETED}",
      "%{TOIL_ISSUED_JOB}",
      "%{TOIL_SAVING_GRAPH}",
      "%{TOIL_SINGLE_MACHINE_CORES}",
      "%{TOIL_NOT_CHAINING}",
      "%{TOIL_STATS_MESSAGE}",
      "%{TOIL_RESOLVED_MESSAGE}",
      "%{TOIL_LOADED_BODY}",
      "%{TOIL_REAL_TIME_SERVER}",
      "%{TOIL_RESULT_PROCESSED}",
      "%{TOIL_RUNNING_AND_PENDING}",
      "%{TOIL_FINISHED}",
      "%{TOIL_TRACEBACK}",
      "%{TOIL_CWL_WARNING}",
      "%{TOIL_CWL_WARNING_2}",
      "%{TOIL_EXTRA_COMMENT}",
      "%{TOIL_EXTRA_COMMENT_2}",
      "%{EMPTY_SPACE}",
      "%{TOIL_WORKER_LOG_HEADER}",
      "%{TOIL_SUCCESSFULLY_COMPLETED}",
      "%{TOIL_OUTPUT_FILES}",
      "%{TOIL_NO_LOG_FAILURE}",
      "%{TOIL_BATCH_SYSTEM_FAILURE}",
      "%{TOIL_BATCH_DESPITE_FAILURE}",
      "%{TOIL_CHAINING_FROM_JOB}",
      "%{TOIL_DOUBLE_MEMORY}",
      "%{TOIL_JOINING_REAL_TIME_LOGGER}",
      "%{LSF_JOB_COMMENT}",
      "%{LSF_JOB_COMMENT_2}",
      "%{LSF_JOB_COMMENT_3}",
      "%{LSF_JOB_SUMMARY}",
      "%{LSF_JOB_SUMMARY_2}",
      "%{LSF_JOB_SUMMARY_3}",
      "%{LSF_USAGE}",
      "%{LSF_EXIT}"
    ]
    }
  }

if [host] {
   mutate {
     remove_field => ["host"]
   }
}
if [STATUS] == "failed"{
   mutate {
     add_field => { "level" => "error" }
   }
}
if [STATUS] == "failure"{
   mutate {
     add_field => { "level" => "error" }
   }
}
if [STATUS] == "errored"{
   mutate {
     add_field => { "level" => "error" }
   }
}
if [ERROR_INFO] or [ERROR_TYPE]{
   mutate {
    add_field => { "level" => "error" }
    }
} 

if [NON_JAVASCRIPT_WORKER_LOG] {
   grok {
     patterns_dir => ["${PATTERNS_PATH}"]
     match => {
       "NON_JAVASCRIPT_WORKER_LOG" => [
         "%{TOIL_WORKER_FAILED}"
       ]
     }
   }
   mutate {
     remove_field => ["NON_JAVASCRIPT_WORKER_LOG"]
   }
 }

if [path] {
  grok {
    match => {
      "path" => ".*work/%{DATA:RUN_ID}/.*log"
    }
  }
  mutate {
    remove_field => ["path"]
  }
}

if [FAILED_RUN_LIST] {
  mutate{
    split => { "FAILED_RUN_LIST" => " '" }
  }
  grok{
   patterns_dir => ["${PATTERNS_PATH}"]
   match => {
   "FAILED_RUN_LIST" => [
   "%{TOIL_SINGLE_FAILED_JOB_META}"
   ]
  }
  }
}
if [SCRIPT_DATA] {
  mutate {
    gsub => ["SCRIPT_DATA", "\n\t\d+\s+", ""]
  }
  mutate {
    gsub => ["SCRIPT_DATA", "\"", "'"]
  }

}
if [COMMAND] {
  mutate {
    gsub => ["COMMAND", "(^\s+|\\\n\t\s+)", ""]
  }

}
if [JOBFILES] {
  mutate{
    split => {"JOBFILES" => "Downloaded file "}
  }
  grok{
    patterns_dir => ["${PATTERNS_PATH}"]
    match => {
      "JOBFILES" => [
        "%{TOIL_WORKER_FILE_NAME}"
      ]
    }
  }
}
if [HOUR]{
  mutate {
    add_field => {
      "timestamp" => "%{DAY}.%{MONTH}.%{YEAR} %{HOUR}:%{MINUTE}:%{SECOND}"
    }
  
  }
  date {
    match => [
      "timestamp",
      "dd.MM.YYYY HH:mm:ss"
    ]
    target => "date_time"
    remove_field => [
      "timestamp",
      "DAY",
      "MONTH",
      "YEAR",
      "HOUR",
      "MINUTE",
      "SECOND"
    ]
  }
}

if [ERROR] {
  mutate {
    remove_tag => ["_grokparsefailure"]
  }
}
if [STATUS] == ""{
  drop {}
}
if "_grokparsefailure" not in [tags] {
  mutate {
    remove_field => ["message"]
  }
}

if[NEW_MEMORY] {
  ruby {
    code => "event.set('MEMORY', (event.get('NEW_MEMORY').to_f / 1000000000).round(1).to_s)"
  }
  mutate {
    remove_field => ["NEW_MEMORY"]
  }
}

mutate {
    add_field => {
        "ddtags" => "${DD_TAGS}"
        "ddsource" => "logstash"
       }
    }

ruby {
    code => '
        event.to_hash.each { |k, v|
            if v.kind_of? String
                if v == ""
                    event.remove(k)
                end
            else
                if v == nil
                    event.remove(k)
                end
            end
        }
    '
}
}

output {

#   if "_grokparsefailure" in [tags] {
#      elasticsearch {
#         hosts => ["${ELASTIC_SEARCH_URL}"]
#         data_stream => "true"
#	 data_stream_dataset => "parse_failure"
#         data_stream_namespace => "toil_parse_failure"
#      }

#   }
#   else if "_groktimeout" in [tags] {
#      elasticsearch {
#         hosts => ["${ELASTIC_SEARCH_URL}"]
#         data_stream => "true"
#         data_stream_dataset => "parse_timeout"
#         data_stream_namespace => "toil_parse_timeout"
#      }

#   }
#   else {
#      elasticsearch {
#         hosts => ["${ELASTIC_SEARCH_URL}"]
#         data_stream => "true"
#         data_stream_dataset => "log_data"
#         data_stream_namespace => "toil_log_data"
#      }

#   }

datadog_logs {
        api_key => "${DATADOG_API_KEY}"
#        host => "HOST"
#        port => "port"
    }
    
if [level] == "error" {
    file {
    path => "${LOGSTASH_OUTPUT_PATH}/logstash_output.json"
    }
  }
#stdout { codec => rubydebug }

}
